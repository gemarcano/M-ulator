# NUMA_compiler
####################################################################################
## Compiler and test case generator for Non-Uniform Memory DNN Accelerator project
 ##
EECS, University of Michigan

# Build
- Run 'make'
- gcc-4.8 may produce runtime error (pretty-printers wouldn't work)

# Run
- Make sure 'dnn.config', 'dnn.weight', 'dnn.input' are in the directory, then run './dnn-compiler'

# Inputs
- Configuration options are set in dnn.config. Refer to 'config.cpp' for default value and explanations 
- DNN weights and offsets are stored in dnn.weight. first line of the file is the total number of layers; then for every layer, the first line is the number of inputs and outpus, then the weights for one input are store line by line, with space between them, and finally the offsets
- Inputs (for FFT or DNN) are stored in dnn.input. first line of the file is the size of elements in inputs 

# Outputs
- dnn.inst are instruction file
- dnn.memory are binary memory file

# Code file structure
- Overall flow: refer to 'dnn_compiler.cpp' and 'dnn_sim.cpp'. 
The compiler starts by initializing all operations ("dnn_sim()"), which initialize all instructions. Then it assign addresses to inputs, outputs and weights for all operations ("assign_all_inputs()" & "assign_all_parameters()"). 
Then it loads the inputs and produce outputs for all operations ("load_inputs()"). 
Finally it prints out stats, instructions, and test case memory files.
- Operations: refer to 'operation.cpp' and 'operation.h'
- Instructions: definitions are in 'common.h' and 'instruction.h'. 
Usually instructions get initialized by its parent operation (generate_instructions()).
During initialization most instruction fields are assigned or allocated (inputs, outputs and parameters) except addresses.
Addresses then get assigned by dnn_sim from eariler operation to later operation sequentially.

# Notes
- Naming convention: usually, 'XXX_size' are measured in elements, 'XXX_space' are measured in words 
- Values (\*inputs, \*parameters) are stored as quantized floating points. 
Print functions ('print_binary_outputs()', 'print_binary_inputs()') are used to get actual fixed point representation in memory, 
- Similiarly, instructions are stored as class objects, and 'print_instruction()' are used to get actual binary instruction representations. 
- Address: there are two type of addressing: virtual and physical. Virtual address starts from close memory to far away memory, physical adddress starts from far aways memory. For instance,
  virtual   -- L0: 0x0000->0x00ff, L1: 0x0100->0x04ff, L2: 0x0500->0x0fff, L3: 0x1000->0x1fff
  physical  -- L0: 0x1f00->0x1fff, L1: 0x1b00->0x1eff, L2: 0x1000->0x1aff, L3: 0x0000->0x0fff
Virtual address are used in distribution inputs (assign_all_inputs(), assign_all_parameters()), and will be translated to physical address when prints to the output (dnn.test, dnn.memory)
- Most zero padding is done implicitly (memory is initilized with all zeros)
